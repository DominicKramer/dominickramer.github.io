<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.dominickramer.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.dominickramer.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-29T06:22:23+00:00</updated><id>https://www.dominickramer.com/feed.xml</id><title type="html">Artificial Brilliance</title><subtitle>Stay tuned for further update </subtitle><entry><title type="html">More Than Just a Language Model</title><link href="https://www.dominickramer.com/blog/2023/more-than-just-a-language-model/" rel="alternate" type="text/html" title="More Than Just a Language Model"/><published>2023-09-24T15:59:00+00:00</published><updated>2023-09-24T15:59:00+00:00</updated><id>https://www.dominickramer.com/blog/2023/more-than-just-a-language-model</id><content type="html" xml:base="https://www.dominickramer.com/blog/2023/more-than-just-a-language-model/"><![CDATA[<p>In my <a href="/blog/2023/simple-chatbot/">previous post</a> I discussed that, at a very high level, one can think of a Large Language Model (LLM) as a machine that takes an input (a prompt) and returns the completion of that input.</p> <p>For example, given the prompt,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is the largest planet in the solar system?
</code></pre></div></div> <p>OpenAI’s GPT-3.5-turbo model returns a completion of</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The largest planet in the solar system is Jupiter.
</code></pre></div></div> <p>Moreover, one can modify the prompt given to the model to get an answer with a specific structure. For example, given the modified prompt (where I am precise to specify that a single word should be returned as the answer),</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is the largest planet in the solar system?  Use one word in your answer.
</code></pre></div></div> <p>the same GPT-3.5-turbo model returns a completion of</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Jupiter.
</code></pre></div></div> <p>This manipulation of prompts to produce better results from the model is known as <em>prompt engineering</em>. However, it would be missing a big part of the potential of LLMs to think that prompt engineering is <em>only</em> about manipulating prompts.</p> <h1 id="what-really-is-a-large-language-model">What Really is a Large Language Model</h1> <p>To get a bigger view of how to think about LLMs, let’s look at the main parts of a traditional computer at a <strong>very</strong> high level.</p> <p>Specifically, a traditional computer has:</p> <ul> <li>A <strong>CPU</strong> for computation</li> <li>Fast <strong>volatile memory</strong> for storing information</li> <li>Slower <strong>persistent memory</strong> for storing memory between program invocations</li> <li>An <strong>interface</strong> such as a <strong>programming lanauge</strong> to instruct the CPU on what things to compute</li> <li><strong>Peripherals</strong> that allow data to be collected from different sources to be used by the CPU</li> <li>A <strong>BUS</strong> for allowing the different components to communicate</li> </ul> <p>This description is very high level and glosses over a lot of technical details, but conveys the general idea of the different parts of a computer.</p> <p>Now consider the following (somewhat contrived) prompt for an LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are a helpful assistant.

Your Knowledge:

* The average speed of a Corgi dog is 23 mph

Your Process:

You use a Thought/Action/Action Input/Observation pattern:
'''
Thought: The thought you are having to solve the task
Action: The action you want to take.  One of [search]
Action Input: The input to the action
Observation: An observation based on the action
'''

(...this Thought/Action/Action Input/Observation pattern can repeat N times)

== TOOLS ==

You have access to the following tools:

Tool 1:
Action: lookup
Action Input: The query to search
Description: Used to search your persistent memory

Tool 2:
Action: search
Action Input: The query to search
Description: Used to search the internet

== OPTIONS ==

Select ONLY ONE option below:

Option 1: To use a tool, you MUST use the following format:
'''
Thought: Because of the my observations, I think ...
Action: MUST be one of [search]
Action Input: The input to the tool
Observation: The observation from the action
'''

Option 2: To output the final answer, you MUST use the following format:
'''
Final Answer: The final answer for the task
'''

Constraints:
* You MUST use any information in Your Knowledge section before using any tools
* You MUST use a tool if the Your Knowledge section doesn't contain the info you need

Your Task:

Determine if a Corgi dog can run faster than a typical human.
</code></pre></div></div> <p>which a LLM completes as</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A Corgi dog can run faster than a typical human.
</code></pre></div></div> <p>This uses the <a href="https://arxiv.org/abs/2210.03629">ReAct</a> framework for structuring prompts to get LLMs to <strong>reason</strong> and <strong>act</strong> to solve the problem given to them.</p> <p>If we look at this prompt we’ll notice a few things:</p> <ol> <li> <p>The LLM is acting like a computational device (similar to a CPU). However, instead of performing arithmetic operations, it is completing input text.</p> </li> <li> <p>The <strong>Your Knowledge</strong> section, and generally the entire prompt acts like <strong>volatile memory</strong></p> </li> <li> <p>The use of ReAct allows the LLM to use tools, in this case the <strong>lookup</strong> tool, which is similar to a traditional computer accessing <strong>persistent storage</strong></p> </li> <li> <p>The description of the prompt, in particular the ReAct framework, acts as an <strong>interface</strong> to instruct the LLM what to compute, very roughly similar to programming languages in traditional computation.</p> </li> <li> <p>The tools that the ReAct framework allows the LLM to use are similar to <strong>peripherals</strong> a traditional computer program can use.</p> </li> <li> <p>A program that invokes the LLM, gives it input, gets its response, and invokes a tool if needed, acts similar to a <strong>BUS</strong> in a traditional computer.</p> </li> </ol> <h1 id="what-does-this-mean">What Does this Mean?</h1> <p>From this, don’t think of a LLM just as a machine that completes input text. Just as thinking a traditional processor is just a machine that can perform arithmetic quickly, thinking of a LLM as a machine that can answer questions, or chat like a person misses the bigger picture of what an LLM can do.</p> <p>Instead, as Andrej Karpathy puts it in his quote below, think of LLMs as a new type of compute resource: <strong>a general-purpose differentiable computer</strong>.</p> <div style="display: flex; justify-content: center;"> <blockquote class="twitter-tweet"><p lang="en" dir="ltr">The Transformer is a magnificient neural network architecture because it is a general-purpose differentiable computer. It is simultaneously:<br/>1) expressive (in the forward pass)<br/>2) optimizable (via backpropagation+gradient descent)<br/>3) efficient (high parallelism compute graph)</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1582807367988654081?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Next, don’t think about prompt engineering as just manipulating prompts to get an LLM to perform the action that you want. Instead, just like programming languages allow thinking at a higher level to instruct a CPU on what to do, <strong>think of prompts as being like a special programming language to interact with the LLM</strong>.</p> <p>That is, although natural language can be used to interface with an LLM, papers about <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>, <a href="https://arxiv.org/abs/2210.03629">ReAct</a>, and other techniques demonstrate that a specific structured language can be used to <em>better interface</em> with LLMs.</p> <p>Thus, just as one needs to understand how computers work to really understand how to utilize a programming language, one must really understand how LLMs work to fully utilize prompts.</p> <p>Last, don’t just think of tools or prompt techniques as interesting hacks, but rather as fitting into the bigger picture that LLMs are a new form a compute.</p> <p>In summary, LLMs <strong>are a new form a compute</strong> and advances in prompt techniques are showing how to best harness the power of that compute. Although this new compute may be analogous to traditional compute, interacting with LLMs requires novel techniques that continue to make research into prompt engineering very exciting.</p> <h1 id="the-future">The Future</h1> <p>Right now LLMs are being used in a lot of places, in particular for knowledge lookup and in chat interfaces. I think this is great, but it makes me think about how when processors were first commercially available, they were used in places where performing a lot of fast arithmetic was needed.</p> <p>I then compare that to where we are now knowing that using a processor for arithemtic is only scratching the surface of what can be done. That is, when processors first came into use, for many people, the idea of the World Wide Web, augmented reality, or even just a computer on every desk was unimaginable.</p> <p>In the same way, I wonder where we will be in 30 years. Will we look back at our use of LLMs now and think, “Wow, the people then didn’t have any idea just what would be possible with what they have just created”?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In my previous post I discussed that, at a very high level, one can think of a Large Language Model (LLM) as a machine that takes an input (a prompt) and returns the completion of that input.]]></summary></entry><entry><title type="html">A Simple Chatbot</title><link href="https://www.dominickramer.com/blog/2023/simple-chatbot/" rel="alternate" type="text/html" title="A Simple Chatbot"/><published>2023-09-22T15:59:00+00:00</published><updated>2023-09-22T15:59:00+00:00</updated><id>https://www.dominickramer.com/blog/2023/simple-chatbot</id><content type="html" xml:base="https://www.dominickramer.com/blog/2023/simple-chatbot/"><![CDATA[<p>In this post we will go over how to create a simple chatbot using OpenAI’s <a href="https://platform.openai.com/docs/api-reference/chat/create">Create chat completion</a> API endpoint.</p> <blockquote> <p>The code for this blog can be found at <a href="https://github.com/artificial-brilliance/simple_chatbot">artificial-brilliance/simple_chatbot</a>.</p> </blockquote> <h2 id="what-is-a-chatbot">What is a chatbot?</h2> <p>Before beginning, it is important to know what we are actually making, i.e. what is a chatbot? We are going to make a command line tool that when runs has a prompt. You can enter anything you want, press enter, and get a response from the computer as if you are talking with another person.</p> <p>Here is an example of what the final result will look like:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ pdm run start</span>
<span class="s">You are chatting with a helpful assistant.  To stop the conversation press Ctrl+C.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Hi</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">Hello! How can I assist you today?</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Can I ask you a question</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">Of course! Feel free to ask me anything you'd like.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">What is a Corgi dog?</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">A Corgi is a type of dog breed that originated in Wales, United Kingdom. There are two main breeds of Corgis</span><span class="err">:</span> <span class="s">the Pembroke Welsh Corgi and the Cardigan Welsh Corgi.</span>

<span class="s">Pembroke Welsh Corgis are known for their short legs, long bodies, and pointy ears. They have a friendly and outgoing personality, and they are often described as intelligent and affectionate. They are also known for their distinctive "smiling" faces.</span>

<span class="s">Cardigan Welsh Corgis, on the other hand, have a similar body shape but with a longer tail. They have rounded ears and are known for their intelligence, loyalty, and versatility. They are often described as being more reserved and cautious compared to Pembroke Welsh Corgis.</span>

<span class="s">Both breeds are herding dogs and were traditionally used to help farmers in Wales with herding cattle and other livestock. Nowadays, Corgis are popular pets known for their playful nature, loyalty, and adorable appearance.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Thanks.  How do you feel?</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">As an AI, I don't have feelings in the same way humans do. However, I'm here to assist you and provide information to the best of my abilities. Is there anything else I can help you with?</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">That makes sense.</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">I'm glad it makes sense to you! If you have any more questions or need further assistance, feel free to ask. I'm here to help!</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span>
</code></pre></div></div> <p>Note that the computers responses can vary.</p> <h2 id="how-is-this-possible">How is this possible?</h2> <p>Not too long ago, the above would have looked magical. What allows it to be created today: Large Language Models (LLMs).</p> <p>At a very high (and hand-wavy) level, you can think of an LLM as a machine where you give it an input sentance and it returns the most likely next word to follow the sentance. Essentially, LLMs are machines that are very good at playing fill in the blank.</p> <p>For example, if someone said “After waking up, the person drank a cup of _____”, the next word they would probably say is “coffee”. However, they could also say “water” or, less likely, even say “orange juice”. From this, you can see that the completion of a sentance can vary, with some completions more likely than others.</p> <p>LLMs are designed so that they can be shown enormous amounts of text, for example all of Wikipedia, and “learn” relationships between words such that when given an input sentance, they do a good job of giving a resonable next word for the sentance (based on the text they were trained on).</p> <h2 id="how-does-this-relate-to-a-chatbot">How does this relate to a chatbot?</h2> <p>From the perspective of an LLM, suppose it has been trained on a lot of text that consists of conversations. That is, it was shown a lot of text of the form “Hi. How are you? I’m doing good …” so that it could learn the relationship between words.</p> <p>In that case, if you gave it an input sentance such as “You are a helpful assistant: Hi.” it may complete that input with “Hello how are you doing?”. This is because, based on the text it has been trained on, and seeing the the context that it is a helpful assistant and the word “Hi”, it is the most likely way to complete the input as “Hello how are you doing?”.</p> <h2 id="how-does-the-llm-build-a-phrase-to-return">How does the LLM build a phrase to return?</h2> <p>Technically, the LLM proposes one word at a time, and different techniques (which I won’t get into here) are used to keep proposing words until a complete phrase is said.</p> <p>That is, if the OpenAI APIs are asked to complete “You are a helpful assistant: Hi.”, at a very high (hand-wavy) level, the LLM would first suggest the completion “You are a helpful assistant: Hi. Hello”. That is, with one additional word added to the text, which is what the model proposes as the next word.</p> <p>Then when given back the text “You are a helpful assistant: Hi. Hello” the model will return “You are a helpful assistant: Hi. Hello how”. Again when given back the text “You are a helpful assistant: Hi. Hello how” it will return “You are a helpful assistant: Hi. Hello how are”.</p> <p>That is, the sentance is being built up by the model word by word until an entire phrase is constructed. How that phrase is constructed (that is, determining when to stop building the phrase) isn’t going to be discussed here, but for now just note that when you ask the OpenAI chat completion API to complete an input message, it returns the entire phrase.</p> <h2 id="what-does-the-conversation-look-like-for-the-llm">What does the conversation look like for the LLM?</h2> <p>Now lets walk through very-roughly what the LLM does when creating the chat in the example above.</p> <p>First the following input text is given to the LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi
</code></pre></div></div> <p>The LLM completes this text as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?
</code></pre></div></div> <p>The user is then given this last response from the LLM (“Hello! How can I assist you today?”) and is asked to provide input. In the case of the example, the user provides “Can I ask you a question”.</p> <p>As such, this is appened to the end of the message being built up and is given to the LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?

User:
Can I ask you a question
</code></pre></div></div> <p>Note the new <code class="language-plaintext highlighter-rouge">User:</code> entry at the end of the message and that the message continues to grow.</p> <p>After the above message is given to the LLM, it returns with a completion of:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?

User:
Can I ask you a question

Assistant:
Of course! Feel free to ask me anything you'd like.
</code></pre></div></div> <p>This process then continues for the whole conversation.</p> <h2 id="utilizing-openais-api">Utilizing OpenAI’s API</h2> <p>OpenAI’s <a href="https://platform.openai.com/docs/api-reference/chat/create">Create chat completion</a> endpoint is a REST endpoint that accepts many parameters. The one that is of interest to us in the <code class="language-plaintext highlighter-rouge">messages</code> parameter that is a list of entries of the form:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="nl">role</span><span class="p">:</span> <span class="dl">"</span><span class="s2">system</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">user</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">assistant</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">function</span><span class="dl">"</span><span class="p">,</span>
  <span class="nx">content</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> <p>That is, the messages represent a conversation, like the one we built above, with the role of <em>system</em>, <em>user</em>, or <em>assistant</em> being something that is explicitly stated. (Do not worry about the function role for this post).</p> <p>For this post we are going to access the Create chat completion API using the official OpenAI Python client, <a href="https://github.com/openai/openai-python">openai</a>. It’s usage looks like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">openai</span>

<span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
<span class="c1"># in the example above, response would be the string
# "Hello! How can I assist you today?"
</span></code></pre></div></div> <p>Don’t worry about the <code class="language-plaintext highlighter-rouge">model</code> parameter which is used to describe which OpenAI LLM to use. Instead, notice the messages align with the long text we built up in the previous section of the form:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
...

User:
...

Assistant:
...
</code></pre></div></div> <p>However, we specify each message separately and explicitly set the role.</p> <p>Thus, if we explicitly wrote out each step of the conversation we built up above in the previous section, the code could look like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">openai</span>

<span class="n">completion1</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>

<span class="c1"># Hello! How can I assist you today?
</span><span class="n">response1</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>

<span class="n">completion2</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">response1</span> <span class="c1"># Hello! How can I assist you today?
</span>    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Can I ask you a question</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>

<span class="c1"># Of course! Feel free to ask me anything you'd like.
</span><span class="n">response2</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
</code></pre></div></div> <p>Having such manual unrolled code doesn’t scale, but illustrates the process. To make the code more robust we dynamically build up the messages as shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">system_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="p">[{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">,</span>
        <span class="p">}]</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">})</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nf">cast</span><span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">completion</span><span class="p">).</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>Here we create a <code class="language-plaintext highlighter-rouge">ChatBot</code> and initialize the conversation as <code class="language-plaintext highlighter-rouge">self.messages</code> with the sytem prompt (i.e. “You are a helpful assistant.”). Then each call to <code class="language-plaintext highlighter-rouge">process</code> is supplied a message from the user, which is appended to the conversation as a user message. Next, the LLM is asked to complete the conversation, and its response is added to the conversation (<code class="language-plaintext highlighter-rouge">self.messages</code>) as an assistant message. Every time the user has an additional message to say, the <code class="language-plaintext highlighter-rouge">process</code> method is called on the <code class="language-plaintext highlighter-rouge">ChatBot</code>.</p> <h2 id="conclusion">Conclusion</h2> <p>Below you will find the final code as of the time of writing this blog entry that has comments to describe the different parts of the code and shows how to use the <code class="language-plaintext highlighter-rouge">ChatBot</code> class and get input from the user. For the most recent version of the code check its repo at <a href="https://github.com/artificial-brilliance/simple_chatbot">artificial-brilliance/simple_chatbot</a>.</p> <p>I would like to give thanks to Simon Willison for their great <a href="https://til.simonwillison.net/llms/python-react-pattern">post</a> on the ReAct pattern that served as inspiration for this post.</p> <p>I hope you found this blog post helpful, and feel free to leave any questions or comments in the question section.</p> <h2 id="the-final-code">The final code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This code is Apache 2 licensed:
# https://www.apache.org/licenses/LICENSE-2.0
</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="n">openai</span>

<span class="c1"># Read the OpenAI API token form the OPENAI_API_KEY environment variable
</span><span class="n">openai</span><span class="p">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">OPENAI_API_KEY</span><span class="sh">"</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A simple chatbot that is able to have a conversation with a human
    by recording the conversation between the chatbot and the human.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="c1"># If a system prompt is specified, then it should be the first
</span>        <span class="c1"># message in the context.  The "system" role is used to specify
</span>        <span class="c1"># system messages.  These messages are helpful in to instruct
</span>        <span class="c1"># the model on how it should behavior or give it background
</span>        <span class="c1"># information relevant to the conversation.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">system_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="p">[{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">,</span>
        <span class="p">}]</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Processes a message from the user, returning the chatbot</span><span class="sh">'</span><span class="s">s response.

        Parameters
        -----------
        message: the message from the user

        Returns
        -------
        The chatbot</span><span class="sh">'</span><span class="s">s response to the user.
        </span><span class="sh">"""</span>

        <span class="c1"># Add the user's message to the context (i.e. the conversation)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">})</span>

        <span class="c1"># Ask the model to complete the conversation.  That is, the
</span>        <span class="c1"># messages up to this point represent the conversation
</span>        <span class="c1"># where the last message in the list is the most recent
</span>        <span class="c1"># message (i.e. the message the user just made).
</span>        <span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
            <span class="c1"># The model GPT-3.5-turbo is used because it works really
</span>            <span class="c1"># well and has a nice price point
</span>            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
            <span class="c1"># The messages are essentially the conversation at this point.
</span>            <span class="c1"># (i.e. the context).  Each entry in the conversation is marked
</span>            <span class="c1"># as being from the user or the assistant via the "role" field.
</span>            <span class="n">messages</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">,</span>
            <span class="c1"># The temperature is used to essentially tell the API how
</span>            <span class="c1"># creative the model can be.  A value of 1 means it can be
</span>            <span class="c1"># really creative.  That is, the same input to the model can
</span>            <span class="c1"># have very different completions.  A value of 0, on the other
</span>            <span class="c1"># hand, means the same input to the model with always return
</span>            <span class="c1"># roughly the same completion (but note that the completions
</span>            <span class="c1"># won't necessary be exactly the same).
</span>            <span class="c1">#
</span>            <span class="c1"># A value of zero is useful for testing code understanding the
</span>            <span class="c1"># model because is reduces one degree of a freedom (the creativity
</span>            <span class="c1"># of the model) which helps in understanding how the model
</span>            <span class="c1"># responds to different inputs.
</span>            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># The completion of the messages is what the model thinks should
</span>        <span class="c1"># be the next part of the conversation.  As such, make sure
</span>        <span class="c1"># to add the response to the list of messages (which records the
</span>        <span class="c1"># conversation) making sure to specify the "assistant" role, which
</span>        <span class="c1"># instructs the ChatCompletion API that the message came from the
</span>        <span class="c1"># assistant.
</span>        <span class="c1">#
</span>        <span class="c1"># Specifically of all of the choices that the model returns for
</span>        <span class="c1"># possible completions for the conversation, we always just use
</span>        <span class="c1"># the first choice.
</span>        <span class="c1">#
</span>        <span class="c1"># Note: The cast to Any is needed because the type of completion
</span>        <span class="c1">#       is "Unknown".
</span>        <span class="n">result</span> <span class="o">=</span> <span class="nf">cast</span><span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">completion</span><span class="p">).</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>

        <span class="c1"># Update the conversation to include the assistant's response making
</span>        <span class="c1"># sure to specify the response is from the assistant.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>

        <span class="c1"># The completion of the conversation up to this point is the chatbot's
</span>        <span class="c1"># response.
</span>        <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">You are chatting with a helpful assistant.  To stop the conversation press Ctrl+C.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">chatbot</span> <span class="o">=</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">[You] </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="p">.</span><span class="nf">process</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Assistant] </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post we will go over how to create a simple chatbot using OpenAI’s Create chat completion API endpoint.]]></summary></entry></feed>