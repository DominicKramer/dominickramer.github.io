<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A Simple Chatbot | Artificial Brilliance</title> <meta name="author" content="Dominic Kramer"> <meta name="description" content="Stay tuned for further update "> <meta name="keywords" content="llm, large-language-model, openai, prompt-engineering, ai, machine-learning, research"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.dominickramer.com/blog/2023/simple-chatbot/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Artificial Brilliance</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Simple Chatbot</h1> <p class="post-meta">September 22, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post we will go over how to create a simple chatbot using OpenAI’s <a href="https://platform.openai.com/docs/api-reference/chat/create" rel="external nofollow noopener" target="_blank">Create chat completion</a> API endpoint.</p> <blockquote> <p>The code for this blog can be found at <a href="https://github.com/artificial-brilliance/simple_chatbot" rel="external nofollow noopener" target="_blank">artificial-brilliance/simple_chatbot</a>.</p> </blockquote> <h2 id="what-is-a-chatbot">What is a chatbot?</h2> <p>Before beginning, it is important to know what we are actually making, i.e. what is a chatbot? We are going to make a command line tool that when runs has a prompt. You can enter anything you want, press enter, and get a response from the computer as if you are talking with another person.</p> <p>Here is an example of what the final result will look like:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ poetry run python3 simple_chatbot/main.py</span>
<span class="s">You are chatting with a helpful assistant.  To stop the conversation press Ctrl+C.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Hi</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">Hello! How can I assist you today?</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Can I ask you a question</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">Of course! Feel free to ask me anything you'd like.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">What is a Corgi dog?</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">A Corgi is a type of dog breed that originated in Wales, United Kingdom. There are two main breeds of Corgis</span><span class="err">:</span> <span class="s">the Pembroke Welsh Corgi and the Cardigan Welsh Corgi.</span>

<span class="s">Pembroke Welsh Corgis are known for their short legs, long bodies, and pointy ears. They have a friendly and outgoing personality, and they are often described as intelligent and affectionate. They are also known for their distinctive "smiling" faces.</span>

<span class="s">Cardigan Welsh Corgis, on the other hand, have a similar body shape but with a longer tail. They have rounded ears and are known for their intelligence, loyalty, and versatility. They are often described as being more reserved and cautious compared to Pembroke Welsh Corgis.</span>

<span class="s">Both breeds are herding dogs and were traditionally used to help farmers in Wales with herding cattle and other livestock. Nowadays, Corgis are popular pets known for their playful nature, loyalty, and adorable appearance.</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">Thanks.  How do you feel?</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">As an AI, I don't have feelings in the same way humans do. However, I'm here to assist you and provide information to the best of my abilities. Is there anything else I can help you with?</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span> <span class="s">That makes sense.</span>
<span class="pi">[</span><span class="nv">Assistant</span><span class="pi">]</span> <span class="s">I'm glad it makes sense to you! If you have any more questions or need further assistance, feel free to ask. I'm here to help!</span>
<span class="pi">[</span><span class="nv">You</span><span class="pi">]</span>
</code></pre></div></div> <p>Note that the computers responses can vary.</p> <h2 id="how-is-this-possible">How is this possible?</h2> <p>Not too long ago, the above would have looked magical. What allows it to be created today: Large Language Models (LLMs).</p> <p>At a very high (and hand-wavy) level, you can think of an LLM as a machine where you give it an input sentance and it returns the most likely next word to follow the sentance. Essentially, LLMs are machines that are very good at playing fill in the blank.</p> <p>For example, if someone said “After waking up, the person drank a cup of _____”, the next word they would probably say is “coffee”. However, they could also say “water” or, less likely, even say “orange juice”. From this, you can see that the completion of a sentance can vary, with some completions more likely than others.</p> <p>LLMs are designed so that they can be shown enormous amounts of text, for example all of Wikipedia, and “learn” relationships between words such that when given an input sentance, they do a good job of giving a resonable next word for the sentance (based on the text they were trained on).</p> <h2 id="how-does-this-relate-to-a-chatbot">How does this relate to a chatbot?</h2> <p>From the perspective of an LLM, suppose it has been trained on a lot of text that consists of conversations. That is, it was shown a lot of text of the form “Hi. How are you? I’m doing good …” so that it could learn the relationship between words.</p> <p>In that case, if you gave it an input sentance such as “You are a helpful assistant: Hi.” it may complete that input with “Hello how are you doing?”. This is because, based on the text it has been trained on, and seeing the the context that it is a helpful assistant and the word “Hi”, it is the most likely way to complete the input as “Hello how are you doing?”.</p> <h2 id="how-does-the-llm-build-a-phrase-to-return">How does the LLM build a phrase to return?</h2> <p>Technically, the LLM proposes one word at a time, and different techniques (which I won’t get into here) are used to keep proposing words until a complete phrase is said.</p> <p>That is, if the OpenAI APIs are asked to complete “You are a helpful assistant: Hi.”, at a very high (hand-wavy) level, the LLM would first suggest the completion “You are a helpful assistant: Hi. Hello”. That is, with one additional word added to the text, which is what the model proposes as the next word.</p> <p>Then when given back the text “You are a helpful assistant: Hi. Hello” the model will return “You are a helpful assistant: Hi. Hello how”. Again when given back the text “You are a helpful assistant: Hi. Hello how” it will return “You are a helpful assistant: Hi. Hello how are”.</p> <p>That is, the sentance is being built up by the model word by word until an entire phrase is constructed. How that phrase is constructed (that is, determining when to stop building the phrase) isn’t going to be discussed here, but for now just note that when you ask the OpenAI chat completion API to complete an input message, it returns the entire phrase.</p> <h2 id="what-does-the-conversation-look-like-for-the-llm">What does the conversation look like for the LLM?</h2> <p>Now lets walk through very-roughly what the LLM does when creating the chat in the example above.</p> <p>First the following input text is given to the LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi
</code></pre></div></div> <p>The LLM completes this text as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?
</code></pre></div></div> <p>The user is then given this last response from the LLM (“Hello! How can I assist you today?”) and is asked to provide input. In the case of the example, the user provides “Can I ask you a question”.</p> <p>As such, this is appened to the end of the message being built up and is given to the LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?

User:
Can I ask you a question
</code></pre></div></div> <p>Note the new <code class="language-plaintext highlighter-rouge">User:</code> entry at the end of the message and that the message continues to grow.</p> <p>After the above message is given to the LLM, it returns with a completion of:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
You are a helpful assistant.

User:
Hi

Assistant:
Hello! How can I assist you today?

User:
Can I ask you a question

Assistant:
Of course! Feel free to ask me anything you'd like.
</code></pre></div></div> <p>This process then continues for the whole conversation.</p> <h2 id="utilizing-openais-api">Utilizing OpenAI’s API</h2> <p>OpenAI’s <a href="https://platform.openai.com/docs/api-reference/chat/create" rel="external nofollow noopener" target="_blank">Create chat completion</a> endpoint is a REST endpoint that accepts many parameters. The one that is of interest to us in the <code class="language-plaintext highlighter-rouge">messages</code> parameter that is a list of entries of the form:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="nl">role</span><span class="p">:</span> <span class="dl">"</span><span class="s2">system</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">user</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">assistant</span><span class="dl">"</span><span class="o">|</span><span class="dl">"</span><span class="s2">function</span><span class="dl">"</span><span class="p">,</span>
  <span class="nx">content</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> <p>That is, the messages represent a conversation, like the one we built above, with the role of <em>system</em>, <em>user</em>, or <em>assistant</em> being something that is explicitly stated. (Do not worry about the function role for this post).</p> <p>For this post we are going to access the Create chat completion API using the official OpenAI Python client, <a href="https://github.com/openai/openai-python" rel="external nofollow noopener" target="_blank">openai</a>. It’s usage looks like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">openai</span>

<span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
<span class="c1"># in the example above, response would be the string
# "Hello! How can I assist you today?"
</span></code></pre></div></div> <p>Don’t worry about the <code class="language-plaintext highlighter-rouge">model</code> parameter which is used to describe which OpenAI LLM to use. Instead, notice the messages align with the long text we built up in the previous section of the form:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System:
...

User:
...

Assistant:
...
</code></pre></div></div> <p>However, we specify each message separately and explicitly set the role.</p> <p>Thus, if we explicitly wrote out each step of the conversation we built up above in the previous section, the code could look like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">openai</span>

<span class="n">completion1</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>

<span class="c1"># Hello! How can I assist you today?
</span><span class="n">response1</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>

<span class="n">completion2</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span>
    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">response1</span> <span class="c1"># Hello! How can I assist you today?
</span>    <span class="p">},</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Can I ask you a question</span><span class="sh">"</span>
    <span class="p">}],</span>
<span class="p">)</span>

<span class="c1"># Of course! Feel free to ask me anything you'd like.
</span><span class="n">response2</span> <span class="o">=</span> <span class="n">completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
</code></pre></div></div> <p>Having such manual unrolled code doesn’t scale, but illustrates the process. To make the code more robust we dynamically build up the messages as shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">system_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="p">[{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">,</span>
        <span class="p">}]</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">})</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nf">cast</span><span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">completion</span><span class="p">).</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>Here we create a <code class="language-plaintext highlighter-rouge">ChatBot</code> and initialize the conversation as <code class="language-plaintext highlighter-rouge">self.messages</code> with the sytem prompt (i.e. “You are a helpful assistant.”). Then each call to <code class="language-plaintext highlighter-rouge">process</code> is supplied a message from the user, which is appended to the conversation as a user message. Next, the LLM is asked to complete the conversation, and its response is added to the conversation (<code class="language-plaintext highlighter-rouge">self.messages</code>) as an assistant message. Every time the user has an additional message to say, the <code class="language-plaintext highlighter-rouge">process</code> method is called on the <code class="language-plaintext highlighter-rouge">ChatBot</code>.</p> <h2 id="the-final-code">The final code</h2> <p>The following is the final code as of the time of writing this blog entry, has comments to describe the different parts of the code, and shows how to use the <code class="language-plaintext highlighter-rouge">ChatBot</code> class and get input from the user. For the most recent version of the code check its repo at <a href="https://github.com/artificial-brilliance/simple_chatbot" rel="external nofollow noopener" target="_blank">artificial-brilliance/simple_chatbot</a>.</p> <p>I hope you found this blog post helpful, and feel free to leave any questions or comments in the question section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="n">openai</span>

<span class="c1"># Read the OpenAI API token form the OPENAI_API_KEY environment variable
</span><span class="n">openai</span><span class="p">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">OPENAI_API_KEY</span><span class="sh">"</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A simple chatbot that is able to have a conversation with a human
    by recording the conversation between the chatbot and the human.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="c1"># If a system prompt is specified, then it should be the first
</span>        <span class="c1"># message in the context.  The "system" role is used to specify
</span>        <span class="c1"># system messages.  These messages are helpful in to instruct
</span>        <span class="c1"># the model on how it should behavior or give it background
</span>        <span class="c1"># information relevant to the conversation.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">system_prompt</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="p">[{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">system_prompt</span><span class="p">,</span>
        <span class="p">}]</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Processes a message from the user, returning the chatbot</span><span class="sh">'</span><span class="s">s response.

        Parameters
        -----------
        message: the message from the user

        Returns
        -------
        The chatbot</span><span class="sh">'</span><span class="s">s response to the user.
        </span><span class="sh">"""</span>

        <span class="c1"># Add the user's message to the context (i.e. the conversation)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">message</span><span class="p">})</span>

        <span class="c1"># Ask the model to complete the conversation.  That is, the
</span>        <span class="c1"># messages up to this point represent the conversation
</span>        <span class="c1"># where the last message in the list is the most recent
</span>        <span class="c1"># message (i.e. the message the user just made).
</span>        <span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="p">.</span><span class="n">ChatCompletion</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
            <span class="c1"># The model GPT-3.5-turbo is used because it works really
</span>            <span class="c1"># well and has a nice price point
</span>            <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">,</span>
            <span class="c1"># The messages are essentially the conversation at this point.
</span>            <span class="c1"># (i.e. the context).  Each entry in the conversation is marked
</span>            <span class="c1"># as being from the user or the assistant via the "role" field.
</span>            <span class="n">messages</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">,</span>
            <span class="c1"># The temperature is used to essentially tell the API how
</span>            <span class="c1"># creative the model can be.  A value of 1 means it can be
</span>            <span class="c1"># really creative.  That is, the same input to the model can
</span>            <span class="c1"># have very different completions.  A value of 0, on the other
</span>            <span class="c1"># hand, means the same input to the model with always return
</span>            <span class="c1"># roughly the same completion (but note that the completions
</span>            <span class="c1"># won't necessary be exactly the same).
</span>            <span class="c1">#
</span>            <span class="c1"># A value of zero is useful for testing code understanding the
</span>            <span class="c1"># model because is reduces one degree of a freedom (the creativity
</span>            <span class="c1"># of the model) which helps in understanding how the model
</span>            <span class="c1"># responds to different inputs.
</span>            <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># The completion of the messages is what the model thinks should
</span>        <span class="c1"># be the next part of the conversation.  As such, make sure
</span>        <span class="c1"># to add the response to the list of messages (which records the
</span>        <span class="c1"># conversation) making sure to specify the "assistant" role, which
</span>        <span class="c1"># instructs the ChatCompletion API that the message came from the
</span>        <span class="c1"># assistant.
</span>        <span class="c1">#
</span>        <span class="c1"># Specifically of all of the choices that the model returns for
</span>        <span class="c1"># possible completions for the conversation, we always just use
</span>        <span class="c1"># the first choice.
</span>        <span class="c1">#
</span>        <span class="c1"># Note: The cast to Any is needed because the type of completion
</span>        <span class="c1">#       is "Unknown".
</span>        <span class="n">result</span> <span class="o">=</span> <span class="nf">cast</span><span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">completion</span><span class="p">).</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>

        <span class="c1"># Update the conversation to include the assistant's response making
</span>        <span class="c1"># sure to specify the response is from the assistant.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>

        <span class="c1"># The completion of the conversation up to this point is the chatbot's
</span>        <span class="c1"># response.
</span>        <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">You are chatting with a helpful assistant.  To stop the conversation press Ctrl+C.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">chatbot</span> <span class="o">=</span> <span class="nc">ChatBot</span><span class="p">(</span><span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sh">"</span><span class="s">[You] </span><span class="sh">"</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="p">.</span><span class="nf">process</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Assistant] </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"DominicKramer/dominickramer.github.io","data-repo-id":"R_kgDOKXD1VQ","data-category":"General","data-category-id":"DIC_kwDOKXD1Vc4CZjhu","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Dominic Kramer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>