<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>More Than Just a Language Model | Artificial Brilliance</title> <meta name="author" content="Dominic Kramer"> <meta name="description" content="Stay tuned for further update "> <meta name="keywords" content="llm, large-language-model, openai, prompt-engineering, ai, machine-learning, research"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.dominickramer.com/blog/2023/more-than-just-a-language-model/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Artificial Brilliance</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">More Than Just a Language Model</h1> <p class="post-meta">September 24, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In my <a href="/blog/2023/simple-chatbot/">previous post</a> I discussed that, at a very high level, one can think of a Large Language Model (LLM) as a machine that takes an input (a prompt) and returns the completion of that input.</p> <p>For example, given the prompt,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is the largest planet in the solar system?
</code></pre></div></div> <p>OpenAI’s GPT-3.5-turbo model returns a completion of</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The largest planet in the solar system is Jupiter.
</code></pre></div></div> <p>Moreover, one can modify the prompt given to the model to get an answer with a specific structure. For example, given the modified prompt (where I am precise to specify that a single word should be returned as the answer),</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is the largest planet in the solar system?  Use one word in your answer.
</code></pre></div></div> <p>the same GPT-3.5-turbo model returns a completion of</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Jupiter.
</code></pre></div></div> <p>This manipulation of prompts to produce better results from the model is known as <em>prompt engineering</em>. However, it would be missing a big part of the potential of LLMs to think that prompt engineering is <em>only</em> about manipulating prompts.</p> <h1 id="what-really-is-a-large-language-model">What Really is a Large Language Model</h1> <p>To get a bigger view of how to think about LLMs, let’s look at the main parts of a traditional computer at a <strong>very</strong> high level.</p> <p>Specifically, a traditional computer has:</p> <ul> <li>A <strong>CPU</strong> for computation</li> <li>Fast <strong>volatile memory</strong> for storing information</li> <li>Slower <strong>persistent memory</strong> for storing memory between program invocations</li> <li>An <strong>interface</strong> such as a <strong>programming lanauge</strong> to instruct the CPU on what things to compute</li> <li> <strong>Peripherals</strong> that allow data to be collected from different sources to be used by the CPU</li> <li>A <strong>BUS</strong> for allowing the different components to communicate</li> </ul> <p>This description is very high level and glosses over a lot of technical details, but conveys the general idea of the different parts of a computer.</p> <p>Now consider the following (somewhat contrived) prompt for an LLM:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are a helpful assistant.

Your Knowledge:

* The average speed of a Corgi dog is 23 mph

Your Process:

You use a Thought/Action/Action Input/Observation pattern:
'''
Thought: The thought you are having to solve the task
Action: The action you want to take.  One of [search]
Action Input: The input to the action
Observation: An observation based on the action
'''

(...this Thought/Action/Action Input/Observation pattern can repeat N times)

== TOOLS ==

You have access to the following tools:

Tool 1:
Action: lookup
Action Input: The query to search
Description: Used to search your persistent memory

Tool 2:
Action: search
Action Input: The query to search
Description: Used to search the internet

== OPTIONS ==

Select ONLY ONE option below:

Option 1: To use a tool, you MUST use the following format:
'''
Thought: Because of the my observations, I think ...
Action: MUST be one of [search]
Action Input: The input to the tool
Observation: The observation from the action
'''

Option 2: To output the final answer, you MUST use the following format:
'''
Final Answer: The final answer for the task
'''

Constraints:
* You MUST use any information in Your Knowledge section before using any tools
* You MUST use a tool if the Your Knowledge section doesn't contain the info you need

Your Task:

Determine if a Corgi dog can run faster than a typical human.
</code></pre></div></div> <p>which a LLM completes as</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A Corgi dog can run faster than a typical human.
</code></pre></div></div> <p>This uses the <a href="https://arxiv.org/abs/2210.03629" rel="external nofollow noopener" target="_blank">ReAct</a> framework for structuring prompts to get LLMs to <strong>reason</strong> and <strong>act</strong> to solve the problem given to them.</p> <p>If we look at this prompt we’ll notice a few things:</p> <ol> <li> <p>The LLM is acting like a computational device (similar to a CPU). However, instead of performing arithmetic operations, it is completing input text.</p> </li> <li> <p>The <strong>Your Knowledge</strong> section, and generally the entire prompt acts like <strong>volatile memory</strong></p> </li> <li> <p>The use of ReAct allows the LLM to use tools, in this case the <strong>lookup</strong> tool, which is similar to a traditional computer accessing <strong>persistent storage</strong></p> </li> <li> <p>The description of the prompt, in particular the ReAct framework, acts as an <strong>interface</strong> to instruct the LLM what to compute, very roughly similar to programming languages in traditional computation.</p> </li> <li> <p>The tools that the ReAct framework allows the LLM to use are similar to <strong>peripherals</strong> a traditional computer program can use.</p> </li> <li> <p>A program that invokes the LLM, gives it input, gets its response, and invokes a tool if needed, acts similar to a <strong>BUS</strong> in a traditional computer.</p> </li> </ol> <h1 id="what-does-this-mean">What Does this Mean?</h1> <p>From this, don’t think of a LLM just as a machine that completes input text. Just as thinking a traditional processor is just a machine that can perform arithmetic quickly, thinking of a LLM as a machine that can answer questions, or chat like a person misses the bigger picture of what an LLM can do.</p> <p>Instead, as Andrej Karpathy puts it in his quote below, think of LLMs as a new type of compute resource: <strong>a general-purpose differentiable computer</strong>.</p> <div style="display: flex; justify-content: center;"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">The Transformer is a magnificient neural network architecture because it is a general-purpose differentiable computer. It is simultaneously:<br>1) expressive (in the forward pass)<br>2) optimizable (via backpropagation+gradient descent)<br>3) efficient (high parallelism compute graph)</p>— Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1582807367988654081?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">October 19, 2022</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Next, don’t think about prompt engineering as just manipulating prompts to get an LLM to perform the action that you want. Instead, just like programming languages allow thinking at a higher level to instruct a CPU on what to do, <strong>think of prompts as being like a special programming language to interact with the LLM</strong>.</p> <p>That is, although natural language can be used to interface with an LLM, papers about <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">Chain-of-Thought</a>, <a href="https://arxiv.org/abs/2210.03629" rel="external nofollow noopener" target="_blank">ReAct</a>, and other techniques demonstrate that a specific structured language can be used to <em>better interface</em> with LLMs.</p> <p>Thus, just as one needs to understand how computers work to really understand how to utilize a programming language, one must really understand how LLMs work to fully utilize prompts.</p> <p>Last, don’t just think of tools or prompt techniques as interesting hacks, but rather as fitting into the bigger picture that LLMs are a new form a compute.</p> <p>In summary, LLMs <strong>are a new form a compute</strong> and advances in prompt techniques are showing how to best harness the power of that compute. Although this new compute may be analogous to traditional compute, interacting with LLMs requires novel techniques that continue to make research into prompt engineering very exciting.</p> <h1 id="the-future">The Future</h1> <p>Right now LLMs are being used in a lot of places, in particular for knowledge lookup and in chat interfaces. I think this is great, but it makes me think about how when processors were first commercially available, they were used in places where performing a lot of fast arithmetic was needed.</p> <p>I then compare that to where we are now knowing that using a processor for arithemtic is only scratching the surface of what can be done. That is, when processors first came into use, for many people, the idea of the World Wide Web, augmented reality, or even just a computer on every desk was unimaginable.</p> <p>In the same way, I wonder where we will be in 30 years. Will we look back at our use of LLMs now and think, “Wow, the people then didn’t have any idea just what would be possible with what they have just created”?</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"DominicKramer/dominickramer.github.io","data-repo-id":"R_kgDOKXD1VQ","data-category":"General","data-category-id":"DIC_kwDOKXD1Vc4CZjhu","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Dominic Kramer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>